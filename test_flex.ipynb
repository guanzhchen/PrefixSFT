{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import (\n",
    "    flex_attention,\n",
    ")\n",
    "import torch\n",
    "flex_attention = torch.compile(flex_attention, dynamic=False)\n",
    "\n",
    "# torch._dynamo.config.cache_size_limit = 192\n",
    "# torch._dynamo.config.accumulated_cache_size_limit = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = flex_attention(query, key, value, block_mask=self.block_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Attention Output Shape: torch.Size([2, 4, 10, 8])\n",
      "Flex Attention Output Shape: torch.Size([2, 4, 10, 8])\n",
      "\n",
      "Difference in Outputs: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.nn.attention.flex_attention import (\n",
    "    create_block_mask,\n",
    ")\n",
    "\n",
    "# from torch.nn.attention import and_masks, or_masks\n",
    "\n",
    "def build_attention_mask_vectorized_simplified(mask):\n",
    "    \"\"\"\n",
    "    Simplified vectorized implementation for masks with pre-labeled chunks.\n",
    "    Args:\n",
    "        mask (torch.Tensor): Shape (B, S), where 0 = padding/invalid, >0 = chunk labels\n",
    "    Returns:\n",
    "        torch.Tensor: Attention mask of shape (B, 1, S, S)\n",
    "    \"\"\"\n",
    "    B, S = mask.shape\n",
    "    device = mask.device\n",
    "\n",
    "    # Expand mask to (B, S, S) for pairwise comparison\n",
    "    chunk_ids = mask.unsqueeze(-1)  # (B, S, 1)\n",
    "    same_chunk = chunk_ids == chunk_ids.transpose(-2, -1)  # (B, S, S)\n",
    "\n",
    "    # Mask out invalid positions (where mask == 0)\n",
    "    valid_token = (mask != 0).unsqueeze(-1)  # (B, S, 1)\n",
    "    attention_mask = (same_chunk & valid_token).to(torch.float32)  # (B, S, S)\n",
    "\n",
    "    # Add head dimension for multi-head attention compatibility\n",
    "    return attention_mask.unsqueeze(1)  # (B, 1, S, S)\n",
    "\n",
    "\n",
    "# Naive PyTorch Attention\n",
    "class NaiveAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(NaiveAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.scale = 1.0 / (d_model ** 0.5)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: [batch_size, seq_len, d_model]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # [batch_size, H, seq_len, seq_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Flex Attention (Custom Attention Mechanism)\n",
    "class FlexAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(FlexAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.scale = 1.0 / (d_model ** 0.5)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: [batch_size, seq_len, d_model]\n",
    "        output = flex_attention(Q, K, V, block_mask=mask)\n",
    "        return output\n",
    "\n",
    "# Create a custom attention mask\n",
    "def create_custom_mask(seq_len, device='cpu'):\n",
    "    mask = torch.ones(seq_len, seq_len, device=device)\n",
    "    mask = torch.tril(mask)  # Lower triangular mask (causal mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "prefix_length = 4\n",
    "def prefix_mask(b, h, q_idx, kv_idx):\n",
    "    return kv_idx <= 4\n",
    "\n",
    "\n",
    "def causal(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "\n",
    "def generate_block_prefix_causal_mask_mod(prompt_mask_4d):\n",
    "    # prompt_mask_4d = build_attention_mask_vectorized_simplified(prompt_mask).numpy()\n",
    "    def inner_causal(b, h, q_idx, kv_idx):\n",
    "        return q_idx >= kv_idx\n",
    "    # def judge_is_prompt(b, h, q_idx, kv_idx):\n",
    "    #     return prompt_mask_4d[b][h][q_idx][kv_idx]\n",
    "    def block_prefix_causal_mask_mod(b, h, q_idx, kv_idx):\n",
    "        is_prompt = prompt_mask_4d[b, h, q_idx, kv_idx]\n",
    "        return is_prompt | inner_causal(b, h, q_idx, kv_idx)\n",
    "    \n",
    "    return block_prefix_causal_mask_mod\n",
    "\n",
    "\n",
    "\n",
    "# Test the attention mechanisms\n",
    "def test_attention():\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 8\n",
    "    heads = 4\n",
    "\n",
    "    # Create random Q, K, V tensors\n",
    "    Q = torch.randn(batch_size, heads, seq_len, d_model)\n",
    "    K = torch.randn(batch_size, heads, seq_len, d_model)\n",
    "    V = torch.randn(batch_size, heads, seq_len, d_model)\n",
    "    # Example usage\n",
    "    prompt_mask = torch.tensor([\n",
    "        [1, 1, 1, 0, 0, 0, 2, 2, 0, 0],\n",
    "        [1, 0, 2, 2, 0, 3, 3, 0, 0, 0]\n",
    "    ], dtype=torch.long)\n",
    "    # Create custom mask\n",
    "    causal = create_custom_mask(seq_len, device=Q.device).expand(batch_size, heads, seq_len, seq_len)\n",
    "    mask = build_attention_mask_vectorized_simplified(prompt_mask)\n",
    "    # print(mask.shape)\n",
    "    mask = mask.expand(batch_size, heads, seq_len, seq_len)\n",
    "    # print(mask)\n",
    "    prompt_mask_4d = (mask>0).to(torch.bool)\n",
    "    # print(prompt_mask_4d[0][0][0][0])\n",
    "    # print(causal(0, 0, 0, 0))\n",
    "    mask_mod = generate_block_prefix_causal_mask_mod(prompt_mask_4d)\n",
    "    block_mask = create_block_mask(mask_mod, None, None, seq_len, seq_len, device='cpu', _compile=False)\n",
    "\n",
    "    # Initialize attention mechanisms\n",
    "    naive_attention = NaiveAttention(d_model)\n",
    "    flex_attention = FlexAttention(d_model)\n",
    "\n",
    "    # Forward pass through both attention mechanisms\n",
    "    # mask = mask.expand(batch_size, heads, seq_len, seq_len)\n",
    "    # print(mask)\n",
    "    naive_output = naive_attention(Q, K, V, mask + causal)\n",
    "    flex_output  = flex_attention(Q, K, V, block_mask)\n",
    "    # print(naive_output)\n",
    "    # Compare the outputs\n",
    "    print(\"Naive Attention Output Shape:\", naive_output.shape)\n",
    "    print(\"Flex Attention Output Shape:\", flex_output.shape)\n",
    "    # print(\"Naive Attention Weights Shape:\", naive_weights.shape)\n",
    "    # print(\"Flex Attention Weights Shape:\", flex_weights.shape)\n",
    "\n",
    "    # Compare the results\n",
    "    print(\"\\nDifference in Outputs:\", torch.sum(naive_output - flex_output))\n",
    "    # print(\"Difference in Weights:\", torch.sum(naive_weights - flex_weights))\n",
    "\n",
    "# Run the test\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_attention_mask_vectorized(mask):\n",
    "    \"\"\"\n",
    "    Vectorized implementation to build attention mask from chunked input mask.\n",
    "    Args:\n",
    "        mask (torch.Tensor): Shape (B, S), where 1 = valid token, 0 = padding/invalid\n",
    "    Returns:\n",
    "        torch.Tensor: Attention mask of shape (B, 1, S, S)\n",
    "    \"\"\"\n",
    "    B, S = mask.shape\n",
    "    device = mask.device\n",
    "    \n",
    "    # 1. Identify chunk starts (0->1 transitions)\n",
    "    padded_mask = torch.nn.functional.pad(mask, (1, 0), value=0)  # Pad left with 0\n",
    "    diff = padded_mask[:, 1:] - padded_mask[:, :-1]  # Find transitions\n",
    "    chunk_starts = (diff == 1)  # Marks start of new chunks\n",
    "\n",
    "    # 2. Create chunk IDs using cumulative sum\n",
    "    chunk_ids = torch.cumsum(chunk_starts, dim=1)  # (B, S)\n",
    "    chunk_ids = chunk_ids * mask  # Zero out invalid positions\n",
    "\n",
    "    # 3. Create attention mask (B, S, S)\n",
    "    same_chunk = chunk_ids.unsqueeze(-1) == chunk_ids.unsqueeze(-2)  # (B, S, S)\n",
    "    valid_token = chunk_ids.unsqueeze(-1) != 0  # (B, S, 1)\n",
    "    attention_mask = (same_chunk & valid_token).to(torch.float32)  # Combine conditions\n",
    "\n",
    "    # Add head dimension for multi-head attention compatibility\n",
    "    return attention_mask.unsqueeze(1)  # (B, 1, S, S)\n",
    "\n",
    "# Example usage\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "attention_mask = build_attention_mask_vectorized(mask)\n",
    "# print(\"Vectorized Attention Mask Shape:\", attention_mask.shape)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "def build_attention_mask_vectorized_simplified(mask):\n",
    "    \"\"\"\n",
    "    Simplified vectorized implementation for masks with pre-labeled chunks.\n",
    "    Args:\n",
    "        mask (torch.Tensor): Shape (B, S), where 0 = padding/invalid, >0 = chunk labels\n",
    "    Returns:\n",
    "        torch.Tensor: Attention mask of shape (B, 1, S, S)\n",
    "    \"\"\"\n",
    "    B, S = mask.shape\n",
    "    device = mask.device\n",
    "\n",
    "    # Expand mask to (B, S, S) for pairwise comparison\n",
    "    chunk_ids = mask.unsqueeze(-1)  # (B, S, 1)\n",
    "    same_chunk = chunk_ids == chunk_ids.transpose(-2, -1)  # (B, S, S)\n",
    "\n",
    "    # Mask out invalid positions (where mask == 0)\n",
    "    valid_token = (mask != 0).unsqueeze(-1)  # (B, S, 1)\n",
    "    attention_mask = (same_chunk & valid_token).to(torch.float32)  # (B, S, S)\n",
    "\n",
    "    # Add head dimension for multi-head attention compatibility\n",
    "    return attention_mask.unsqueeze(1)  # (B, 1, S, S)\n",
    "\n",
    "# Example usage\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0, 0, 2, 2, 0, 0, 0],\n",
    "    [1, 0, 2, 2, 0, 3, 3, 0, 0, 0, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "attention_mask = build_attention_mask_vectorized(mask)\n",
    "# print(\"Vectorized Attention Mask Shape:\", attention_mask.shape)\n",
    "print(attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
